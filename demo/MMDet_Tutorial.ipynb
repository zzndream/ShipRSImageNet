{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJxJHruNLb7Y"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ZwwWayne/mmdetection/blob/update-colab/demo/MMDet_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGYwt_UjIrqp"
   },
   "source": [
    "# MMDetection Tutorial\n",
    "\n",
    "Welcome to MMDetection! This is the official colab tutorial for using MMDetection. In this tutorial, you will learn\n",
    "- Perform inference with a MMDet detector.\n",
    "- Train a new detector with a new dataset.\n",
    "\n",
    "Let's start!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi4LPmsR66sy",
    "outputId": "8eb8aadf-1c70-42dd-9105-1a3dad85c504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Wed_Oct_23_19:24:38_PDT_2019\r\n",
      "Cuda compilation tools, release 10.2, V10.2.89\r\n",
      "gcc (GCC) 5.4.0\r\n",
      "Copyright © 2015 Free Software Foundation, Inc.\r\n",
      "本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；\r\n",
      "包括没有适销性和某一专用目的下的适用性担保。\r\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkGnB9WyHSXB",
    "outputId": "f1360573-c24a-4a8f-98cd-cc654c1d7d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\n",
      "Collecting torch==1.5.1+cu101\r\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (704.4 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 704.4 MB 888 bytes/s a 0:00:01     |████████████████████████████▊   | 632.2 MB 16.6 MB/s eta 0:00:05\r\n",
      "\u001B[?25hCollecting torchvision==0.6.1+cu101\r\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 6.6 MB 11.1 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from torch==1.5.1+cu101) (1.20.2)\r\n",
      "Collecting future\r\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from torchvision==0.6.1+cu101) (8.2.0)\r\n",
      "Building wheels for collected packages: future\r\n",
      "  Building wheel for future (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001B[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=40431b77b5ed78d6d943e475095dd16dc24eb9f6229f3319e832165eb2717a73\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\r\n",
      "Successfully built future\r\n",
      "Installing collected packages: future, torch, torchvision\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.7.1\r\n",
      "    Uninstalling torch-1.7.1:\r\n",
      "      Successfully uninstalled torch-1.7.1\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.8.2\r\n",
      "    Uninstalling torchvision-0.8.2:\r\n",
      "      Successfully uninstalled torchvision-0.8.2\r\n",
      "Successfully installed future-0.18.2 torch-1.5.1+cu101 torchvision-0.6.1+cu101\r\n",
      "\u001B[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001B[0m\r\n",
      "Requirement already satisfied: mmcv-full in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (1.3.4)\r\n",
      "Requirement already satisfied: numpy in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from mmcv-full) (1.20.2)\r\n",
      "Requirement already satisfied: Pillow in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from mmcv-full) (8.2.0)\r\n",
      "Requirement already satisfied: yapf in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from mmcv-full) (0.31.0)\r\n",
      "Requirement already satisfied: opencv-python>=3 in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from mmcv-full) (4.5.2.52)\r\n",
      "Requirement already satisfied: pyyaml in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from mmcv-full) (5.4.1)\r\n",
      "Requirement already satisfied: addict in /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages (from mmcv-full) (2.4.0)\r\n",
      "\u001B[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001B[0m\r\n",
      "正克隆到 'mmdetection'...\r\n",
      "fatal: unable to access 'https://github.com/open-mmlab/mmdetection.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\n",
      "[Errno 2] No such file or directory: 'mmdetection'\n",
      "/home/zhangzhn/workspace/jupyter-notebook\n",
      "\u001B[31mERROR: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode: /home/zhangzhn/workspace/jupyter-notebook\u001B[0m\r\n",
      "Collecting Pillow==7.0.0\r\n",
      "  Downloading Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 2.1 MB 820 kB/s eta 0:00:01\r\n",
      "\u001B[?25hInstalling collected packages: Pillow\r\n",
      "  Attempting uninstall: Pillow\r\n",
      "    Found existing installation: Pillow 8.2.0\r\n",
      "    Uninstalling Pillow-8.2.0:\r\n",
      "      Successfully uninstalled Pillow-8.2.0\r\n",
      "Successfully installed Pillow-7.0.0\r\n",
      "\u001B[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
    "!pip install -U torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# install mmcv-full thus we could use CUDA operators\n",
    "!pip install mmcv-full\n",
    "\n",
    "# Install mmdetection\n",
    "!rm -rf mmdetection\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "\n",
    "!pip install -e .\n",
    "\n",
    "# install Pillow 7.0.0 back in order to avoid bug in colab\n",
    "!pip install Pillow==7.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hD0mmMixT0p",
    "outputId": "5316598c-233a-4140-db12-64d3a0df216b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1+cu101 True\n",
      "2.11.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages/mmcv/_ext.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at5emptyEN3c108ArrayRefIlEERKNS0_13TensorOptionsENS0_8optionalINS0_12MemoryFormatEEE",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-228763291d62>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m# Check mmcv installation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mmmcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mops\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mget_compiling_cuda_version\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mget_compiler_version\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_compiling_cuda_version\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_compiler_version\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/zhangzhn/lib/python3.7/site-packages/mmcv/ops/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mbbox\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mbbox_overlaps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mbox_iou_rotated\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mbox_iou_rotated\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mcarafe\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCARAFE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCARAFENaive\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCARAFEPack\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcarafe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcarafe_naive\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mcc_attention\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCrissCrossAttention\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mcontour_expand\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcontour_expand\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/zhangzhn/lib/python3.7/site-packages/mmcv/ops/bbox.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mext_loader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mext_module\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mext_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_ext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'_ext'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'bbox_overlaps'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/zhangzhn/lib/python3.7/site-packages/mmcv/utils/ext_loader.py\u001B[0m in \u001B[0;36mload_ext\u001B[0;34m(name, funcs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mload_ext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0mext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimport_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'mmcv.'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mfun\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf'{fun} miss in module {name}'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/zhangzhn/lib/python3.7/importlib/__init__.py\u001B[0m in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m             \u001B[0mlevel\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_bootstrap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gcd_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: /root/anaconda3/envs/zhangzhn/lib/python3.7/site-packages/mmcv/_ext.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at5emptyEN3c108ArrayRefIlEERKNS0_13TensorOptionsENS0_8optionalINS0_12MemoryFormatEEE"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(mmdet.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi9zw03oM4CH"
   },
   "source": [
    "## Perform inference with a MMDet detector\n",
    "MMDetection already provides high level APIs to do inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4doHX4exvS1",
    "outputId": "688ef595-5742-4210-90d0-b841044a7892"
   },
   "outputs": [],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth \\\n",
    "      -O checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8M5KUnX7Np3h"
   },
   "outputs": [],
   "source": [
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "\n",
    "# Choose to use a config and initialize the detector\n",
    "config = 'configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "# initialize the detector\n",
    "model = init_detector(config, checkpoint, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi6DRpsQPEmV",
    "outputId": "8ea1de7e-d20f-44cf-9967-24578d51ff16"
   },
   "outputs": [],
   "source": [
    "# Use the detector to do inference\n",
    "img = 'demo/demo.jpg'\n",
    "result = inference_detector(model, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "UsJU5D-QPX8L",
    "outputId": "04df7cef-6393-4147-da43-ab89f3b29a56"
   },
   "outputs": [],
   "source": [
    "# Let's plot the result\n",
    "show_result_pyplot(model, img, result, score_thr=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GrWIJywLV-V"
   },
   "source": [
    "## Train a detector on customized dataset\n",
    "\n",
    "To train a new detector, there are usually three things to do:\n",
    "1. Support a new dataset\n",
    "2. Modify the config\n",
    "3. Train a new detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E73y5Lru-wBx"
   },
   "source": [
    "### Support a new dataset\n",
    "\n",
    "There are three ways to support a new dataset in MMDetection: \n",
    "  1. reorganize the dataset into COCO format.\n",
    "  2. reorganize the dataset into a middle format.\n",
    "  3. implement a new dataset.\n",
    "\n",
    "Usually we recommend to use the first two methods which are usually easier than the third.\n",
    "\n",
    "In this tutorial, we gives an example that converting the data into the format of existing datasets like COCO, VOC, etc. Other methods and more advanced usages can be found in the [doc](https://mmdetection.readthedocs.io/en/latest/tutorials/new_dataset.html#).\n",
    "\n",
    "Firstly, let's download a tiny dataset obtained from [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d). We select the first 75 images and their annotations from the 3D object detection dataset (it is the same dataset as the 2D object detection dataset but has 3D annotations). We convert the original images from PNG to JPEG format with 80% quality to reduce the size of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHnw5Q_nARXq",
    "outputId": "a61e0685-6441-4ff2-994a-15da68e507fe"
   },
   "outputs": [],
   "source": [
    "# download, decompress the data\n",
    "!wget https://download.openmmlab.com/mmdetection/data/kitti_tiny.zip\n",
    "!unzip kitti_tiny.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wuwxw1oZRtVZ",
    "outputId": "7f88e82a-0825-4c9e-e584-bd43589feeaf"
   },
   "outputs": [],
   "source": [
    "# Check the directory structure of the tiny data\n",
    "\n",
    "# Install tree first\n",
    "!apt-get -q install tree\n",
    "!tree kitti_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "YnQQqzOWzE91",
    "outputId": "455b3e61-0463-4dc5-e21f-17dd204938fb"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the dataset image\n",
    "import mmcv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = mmcv.imread('kitti_tiny/training/image_2/000073.jpeg')\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(mmcv.bgr2rgb(img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMZvtSIl71qi"
   },
   "source": [
    "After downloading the data, we need to implement a function to convert the kitti annotation format into the middle format. In this tutorial we choose to convert them in **`load_annotations`** function in a newly implemented **`KittiTinyDataset`**.\n",
    "\n",
    "Let's take a loot at the annotation txt file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7rwalnPd6e1",
    "outputId": "539d4183-cae3-4485-f894-772c334b613f"
   },
   "outputs": [],
   "source": [
    "# Check the label of a single image\n",
    "!cat kitti_tiny/training/label_2/000000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA1pFg-FeO3l"
   },
   "source": [
    "According to the KITTI's documentation, the first column indicates the class of the object, and the 5th to 8th columns indicates the bboxes. We need to read annotations of each image and convert them into middle format MMDetection accept is as below:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        'filename': 'a.jpg',\n",
    "        'width': 1280,\n",
    "        'height': 720,\n",
    "        'ann': {\n",
    "            'bboxes': <np.ndarray> (n, 4),\n",
    "            'labels': <np.ndarray> (n, ),\n",
    "            'bboxes_ignore': <np.ndarray> (k, 4), (optional field)\n",
    "            'labels_ignore': <np.ndarray> (k, 4) (optional field)\n",
    "        }\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdSaB2ad0EdX"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "\n",
    "from mmdet.datasets.builder import DATASETS\n",
    "from mmdet.datasets.custom import CustomDataset\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class KittiTinyDataset(CustomDataset):\n",
    "\n",
    "    CLASSES = ('Car', 'Pedestrian', 'Cyclist')\n",
    "\n",
    "    def load_annotations(self, ann_file):\n",
    "        cat2label = {k: i for i, k in enumerate(self.CLASSES)}\n",
    "        # load image list from file\n",
    "        image_list = mmcv.list_from_file(self.ann_file)\n",
    "    \n",
    "        data_infos = []\n",
    "        # convert annotations to middle format\n",
    "        for image_id in image_list:\n",
    "            filename = f'{self.img_prefix}/{image_id}.jpeg'\n",
    "            image = mmcv.imread(filename)\n",
    "            height, width = image.shape[:2]\n",
    "    \n",
    "            data_info = dict(filename=f'{image_id}.jpeg', width=width, height=height)\n",
    "    \n",
    "            # load annotations\n",
    "            label_prefix = self.img_prefix.replace('image_2', 'label_2')\n",
    "            lines = mmcv.list_from_file(osp.join(label_prefix, f'{image_id}.txt'))\n",
    "    \n",
    "            content = [line.strip().split(' ') for line in lines]\n",
    "            bbox_names = [x[0] for x in content]\n",
    "            bboxes = [[float(info) for info in x[4:8]] for x in content]\n",
    "    \n",
    "            gt_bboxes = []\n",
    "            gt_labels = []\n",
    "            gt_bboxes_ignore = []\n",
    "            gt_labels_ignore = []\n",
    "    \n",
    "            # filter 'DontCare'\n",
    "            for bbox_name, bbox in zip(bbox_names, bboxes):\n",
    "                if bbox_name in cat2label:\n",
    "                    gt_labels.append(cat2label[bbox_name])\n",
    "                    gt_bboxes.append(bbox)\n",
    "                else:\n",
    "                    gt_labels_ignore.append(-1)\n",
    "                    gt_bboxes_ignore.append(bbox)\n",
    "\n",
    "            data_anno = dict(\n",
    "                bboxes=np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n",
    "                labels=np.array(gt_labels, dtype=np.long),\n",
    "                bboxes_ignore=np.array(gt_bboxes_ignore,\n",
    "                                       dtype=np.float32).reshape(-1, 4),\n",
    "                labels_ignore=np.array(gt_labels_ignore, dtype=np.long))\n",
    "\n",
    "            data_info.update(ann=data_anno)\n",
    "            data_infos.append(data_info)\n",
    "\n",
    "        return data_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwqJOpBe-bMj"
   },
   "source": [
    "### Modify the config\n",
    "\n",
    "In the next step, we need to modify the config for the training.\n",
    "To accelerate the process, we finetune a detector using a pre-trained detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hamZrlnH-YDD"
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HntziLGq-92Z"
   },
   "source": [
    "Given a config that trains a Faster R-CNN on COCO dataset, we need to modify some values to use it for training Faster R-CNN on KITTI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUbwD8uV0PR8",
    "outputId": "43e76fd7-c74b-4ac8-c8b5-4d2cc94e610b"
   },
   "outputs": [],
   "source": [
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'KittiTinyDataset'\n",
    "cfg.data_root = 'kitti_tiny/'\n",
    "\n",
    "cfg.data.test.type = 'KittiTinyDataset'\n",
    "cfg.data.test.data_root = 'kitti_tiny/'\n",
    "cfg.data.test.ann_file = 'train.txt'\n",
    "cfg.data.test.img_prefix = 'training/image_2'\n",
    "\n",
    "cfg.data.train.type = 'KittiTinyDataset'\n",
    "cfg.data.train.data_root = 'kitti_tiny/'\n",
    "cfg.data.train.ann_file = 'train.txt'\n",
    "cfg.data.train.img_prefix = 'training/image_2'\n",
    "\n",
    "cfg.data.val.type = 'KittiTinyDataset'\n",
    "cfg.data.val.data_root = 'kitti_tiny/'\n",
    "cfg.data.val.ann_file = 'val.txt'\n",
    "cfg.data.val.img_prefix = 'training/image_2'\n",
    "\n",
    "# modify num classes of the model in box head\n",
    "cfg.model.roi_head.bbox_head.num_classes = 3\n",
    "# We can still use the pre-trained Mask RCNN model though we do not need to\n",
    "# use the mask branch\n",
    "cfg.load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './tutorial_exps'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.optimizer.lr = 0.02 / 8\n",
    "cfg.lr_config.warmup = None\n",
    "cfg.log_config.interval = 10\n",
    "\n",
    "# Change the evaluation metric since we use customized dataset.\n",
    "cfg.evaluation.metric = 'mAP'\n",
    "# We can set the evaluation interval to reduce the evaluation times\n",
    "cfg.evaluation.interval = 12\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 12\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "111W_oZV_3wa"
   },
   "source": [
    "### Train a new detector\n",
    "\n",
    "Finally, lets initialize the dataset and detector, then train a new detector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c3018c8715924d2b83d817cc6c448a2d",
      "aca1c388eeca4c87b5b6306302630303",
      "b9b75e2d894e467289cb83070b8bb998",
      "767c8f4fbc924027885851365ceb6292",
      "1489fe29d91748cab449718d687f4ee1",
      "bf1e5d0665a141ac9c2085062ba77801",
      "171ea927699a474084c49f8874942ae8",
      "7189ce8a6634410a9e633832e8151070"
     ]
    },
    "id": "7WBWHu010PN3",
    "outputId": "fac18cba-16a3-491e-b3ae-1ab99d71c325"
   },
   "outputs": [],
   "source": [
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector\n",
    "\n",
    "\n",
    "# Build dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the detector\n",
    "model = build_detector(\n",
    "    cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vYQF5K2NqqI"
   },
   "source": [
    "### Understand the log\n",
    "From the log, we can have a basic understanding the training process and know how well the detector is trained.\n",
    "\n",
    "Firstly, the ResNet-50 backbone pre-trained on ImageNet is loaded, this is a common practice since training from scratch is more cost. The log shows that all the weights of the ResNet-50 backbone are loaded except the `conv1.bias`, which has been merged into `conv.weights`.\n",
    "\n",
    "Second, since the dataset we are using is small, we loaded a Mask R-CNN model and finetune it for detection. Because the detector we actually using is Faster R-CNN, the weights in mask branch, e.g. `roi_head.mask_head`, are `unexpected key in source state_dict` and not loaded.\n",
    "The original Mask R-CNN is trained on COCO dataset which contains 80 classes but KITTI Tiny dataset only have 3 classes. Therefore, the last FC layer of the pre-trained Mask R-CNN for classification has different weight shape and is not used.\n",
    "\n",
    "Third, after training, the detector is evaluated by the default VOC-style evaluation. The results show that the detector achieves 54.1 mAP on the val dataset,\n",
    " not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfQ-yspZLuuI"
   },
   "source": [
    "## Test the trained detector\n",
    "\n",
    "After finetuning the detector, let's visualize the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "_MuZurfGLq0p",
    "outputId": "b4a77811-d159-4213-d8cb-b73f5b5b6d1c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = mmcv.imread('kitti_tiny/training/image_2/000068.jpeg')\n",
    "\n",
    "model.cfg = cfg\n",
    "result = inference_detector(model, img)\n",
    "show_result_pyplot(model, img, result)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MMDet Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "zhangzhn",
   "language": "python",
   "display_name": "zhangzhn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1489fe29d91748cab449718d687f4ee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "171ea927699a474084c49f8874942ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7189ce8a6634410a9e633832e8151070": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "767c8f4fbc924027885851365ceb6292": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7189ce8a6634410a9e633832e8151070",
      "placeholder": "​",
      "style": "IPY_MODEL_171ea927699a474084c49f8874942ae8",
      "value": " 89.9M/89.9M [00:11&lt;00:00, 8.22MB/s]"
     }
    },
    "aca1c388eeca4c87b5b6306302630303": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9b75e2d894e467289cb83070b8bb998": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf1e5d0665a141ac9c2085062ba77801",
      "max": 94284731,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1489fe29d91748cab449718d687f4ee1",
      "value": 94284731
     }
    },
    "bf1e5d0665a141ac9c2085062ba77801": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3018c8715924d2b83d817cc6c448a2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9b75e2d894e467289cb83070b8bb998",
       "IPY_MODEL_767c8f4fbc924027885851365ceb6292"
      ],
      "layout": "IPY_MODEL_aca1c388eeca4c87b5b6306302630303"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}